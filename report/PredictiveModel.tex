\paragraph{}
L'objectif général qui nous a été donné était de lier une émotion à une
interaction avec le robot. Pour ce faire, il ne suffit pas de simuler des
émotions, il est aussi nécessaire de les faire correspondre à un événement
identifiable par les personnes qui sont en train d'interagir avec lui. Il est
donc nécessaire de produire une forme d'association entre les différents
stimuli possibles et les actions simulant des émotions.

\subsubsection{Contexte des émotions}
En analysant les descriptions des contextes d'apparition des différentes
émotions, nous avons pu classifier les comportements nécessaires en trois
catégories distinctes.
% 3 paragraphes distincts?
\begin{itemize}
\item Les comportements de réactivité
      \begin{itemize}
      \item L'excitation dans le cas d'un stimulus positif
      \item La crainte dans le cas d'un stimulus négatif
      \end{itemize}
\item Les comportements de prédiction
      \begin{itemize}
      \item L'excitation dans le cas d'une anticipation de stimulus positif
      \item La crainte dans le cas d'une anticipation de stimulus négatif
      \end{itemize}
\item Les comportements de correction de prédiction
      \begin{itemize}
      \item Le soulagement dans le cas d'une anticipation négative non réalisée
      \item La tristesse dans le cas d'une anticipation positive non réalisée
      \end{itemize}
\end{itemize}

Afin de rendre les prédictions crédibles aux yeux d'un humain, il est
donc nécessaire de créer un module permettant d'évaluer la probabilité que le
prochain stimulus soit positif ou négatif en connaissant le dernier stimuli
perçu. Ce module doit pouvoir apprendre cette relation en se basant sur les
expériences qu'il a pu faire. Il doit aussi être capable d'adapter ses
connaissances facilement afin de faire face à un changement d'environnement.

%TODO décrire les modèles évoqué en cours (Rescorla Wagner)

\subsubsection{Apprentissage naïf}
\paragraph{}
Lorsque l'on souhaite apprendre un modèle sans disposer d'un échantillon
préalable conséquent, il est nécessaire de prendre certaines précautions.
Effectivement, si l'on lance un dé deux fois de suite et que l'on obtient deux
6, cela ne suffit pas à conclure que la probabilité d'obtenir un 6 sur ce dé
est supérieure à $\frac{1}{2}$. Pourtant, si l'on se base sur une estimation
simpliste des probabilités, on pourrait approximer cette probabilité par la
valeur 1.

\paragraph{}
En notant $E$ l'ensemble des expériences et $E_a$ le sous-ensemble de $E$
respectant la propriété $a$, cette estimation de probabilité naïve se définit
ainsi.

$$P(a) = \frac{|E_a|}{|E|}$$

\subsubsection{Lissage laplacien}
\paragraph{}
Le lissage laplacien permet d'éviter de favoriser des événements trop
fortement lorsque le nombre d'expériences est faible. Le principe utilisé est
de définir l'espace des possibilités et d'ajouter $k$ expérience pour chacune
d'entre elles. Dans le cas du dé, il y aurait donc 6 résultats possibles. Ce
nombre total de possibilité sera noté $n$. La formule associée au lissage
laplacien devient donc la suivante.

$$P(a) = \frac{|E_a| + k}{|E| + k * n} $$

\subsubsection{Modèle probabiliste final}
\paragraph{}
Afin de rendre plus dynamique notre modèle d'apprentissage et de faciliter la
mise à jour des connaissances, nous avons décidé d'utiliser une version
enrichie du modèle Laplacien. Effectivement, au bout d'un certain temps,
le nombre d'expériences accumulées devient si grand que si l'on essaye de lui
faire changer son modèle sans le réinitialiser, il faudra un grand nombre
d'expériences. Afin d'éviter cela, nous avons décider de prendre en compte la
temporalité des événements

%Caractéristiques attendues
\paragraph{}
Les caractéristiques que nous souhaitions remplir avec notre modèle prédictif
étaient les suivantes :
\begin{itemize}
\item Favoriser les éléments récents, particulièrement à court-terme
\item Prendre en compte le nombre d'occurrence des expériences,
      particulièrement à moyen terme
\item Oublier les expériences très vieilles, en convergeant à long terme vers
      une loi uniforme, cette loi uniforme doit aussi être présente dans le
      cas d'une absence totale d'expériences.
\end{itemize}

% Experience
\paragraph{}
Nous avons donc décidé de définir une expérience sous
la forme suivante~:
$$x = \{s, v, t\}$$
\begin{itemize}
\item $s(x)$ le stimulus précédent $x$
\item $v(x)$ la valeur associée à $x$
\item $t(x)$ le temps auquel s'est produit $x$
\end{itemize}

% Ensembles
\paragraph{}
Dans le cadre de nos expériences, il existe plusieurs ensembles qui vont
servir de base à l'évaluation des expériences~:
\begin{itemize}
\item $S=\{caresse, claque, mouchoir, ...\}$~: L'ensemble des stimuli.
\item $V=\{+,-,rien\}$~: Les valeurs associées aux stimuli.
\item $E$~: L'ensemble des expériences
\item $E(s)$ avec $s \in S$~: L'ensemble des expériences précédées par le
      stimulus $s$.
\item $E(s,v)$ avec $s \in S$ et $v \in V$~: L'ensemble des expériences
      précédées par le stimulus $s$ avec comme valeur associée $v$.
\end{itemize}

% Probabilités
\paragraph{}
On notera:
%TODO maybe find a better notation for probability dependent of time
\begin{itemize}
\item $P(s, t)$~: La probabilité du stimulus $s$ à l'instant $t$.
\item $P(v|s, t)$~: La probabilité que la valeur $v$ apparaisse en fonction
      de $t$, sachant que le dernier stimulus perçu était $s$.
\end{itemize}
\paragraph{}
Afin de débuter par une loi uniforme, nous avons gardé des lois ayant une
forme commune avec celle du lissage de Laplace. Celles-ci repose sur une
fonction de score et peuvent être décrites ainsi~:
$$P(s,t) = \frac{k + \sum\limits_{x \in E(s)} \mathrm{score}(x, t)}
                {|S| * k + \sum\limits_{x \in E} \mathrm{score}(x, t)}$$
$$P(v| s, t) = \frac{k + \sum\limits_{x \in E(s, v)} \mathrm{score}(x, t)}
                {|V| * k + \sum\limits_{x \in E(s)} \mathrm{score}(x, t)}$$

\paragraph{}
L'avantage de ces formules est que si on définit
$\forall x \in E, \mathrm{score}(x, t) = 1$, alors les distributions sont
exactement les mêmes que dans le cas d'un lissage Laplacien. En revanche, si
l'ont définit la fonction de score ainsi~:
$\forall x \in E, \mathrm{score}(x, t) = e^{-(t - t(x))}$ il devient évident que
les événements récents auront plus de poids que les vieux. Cependant, cette
fonction ne nous permettait pas d'obtenir un des aspects que nous souhaitions,
l'importance du nombre d'expérience à moyen terme. Effectivement en l'absence
de nouvelles expériences, tous les scores décroîtront, laissant ainsi la loi
tendre vers une loi uniforme.

\paragraph{}
Afin d'obtenir cet effet par le nombre d'expériences à moyen terme sans perdre
l'effet de précédence à court terme, nous avons réfléchi à la fonction de
score idéale à utiliser. Nous avons finalement abouti à la fonction suivante~:
$e^{-(\frac{t-t(x)}{\tau})^{\alpha}}$

\paragraph{}
Cette fonction utilise un $\alpha < 1$ afin de diminuer le rapport~:
$\frac{\mathrm{score}(x,t)}{\mathrm{score}(x, t + dt)}$ à moyen terme, sans
pour autant l'affecter outre mesure à court terme. La constante $\tau$ permet
de régler l'écoulement du temps au vu du score, elle peut servir pour
déterminer l'échelle de la mémoire, afin de définir si le robot aura une
mémoire qui s'étale sur des secondes ou des semaines.

% ajout de graphiques

\paragraph{}
Le $k$ du lissage laplacien qui avait une valeur sémantique dans l'ancien
modèle\footnote{Il représentait une initialisation avec $k$ expériences de
chaque type de résultat possible.} n'est à présent plus très explicite et
devient donc difficile à régler afin d'obtenir le résultat désiré. Cependant,
ce $k$ étant une constante, il est parfaitement possible de l'exprimer
autrement. Il devient beaucoup plus facile à utiliser en l'exprimant ainsi~:
$$k = e^{-\left (\frac{k_t}{\tau} \right )^{\alpha}}$$

\paragraph{}
Même si l'expression du $k$ ainsi semble rendre l'objet plus complexe, il est
ensuite beaucoup plus facile de comprendre sa signification dans la nouvelle
formule. Imaginons une expérience fantôme $x_f$ tel que
$\forall t, t - t(x_f) = k_t$, le score de cette expérience serait donc
exactement le même que celui d'une expérience s'étant déroulée $k_t$ avant
l'instant actuel. Sémantiquement le sens de ce k est donc devenu l'ajout d'une
expérience virtuelle qui s'est déroulée $k_t$ avant l'instant auquel on évalue
la probabilité.

\paragraph{}
Étant donné que nous avons remplacé une constante par une autre, cette
dernière modification n'affecte absolument pas les propriétés que nous avons
attribuée au système. En revanche, le réglage devient beaucoup plus facile
étant donné que l'influence de $k_t$ est directement compréhensible alors
que faire le lien entre une valeur de $k$ donnée et le moment de son entrée en
action est beaucoup plus difficile.


\begin{figure}[H]
  \caption{Comparaison de l'évolution des probabilités en fonction de $k_t$}
  \includegraphics[width=\textwidth]{comparison.png}
\end{figure}

% Liste de stimuli dynamique
\paragraph{}
Afin de rendre l'utilisation de notre modèle plus générique, nous avons utilisé
des variables globales définissant les stimuli connus, et à chaque fois qu'un
nouveau stimulus est perçu, nous l'ajoutons à cette liste, sans que cela ne
perturbe notre modèle. Cette aspect permet entre autre de pouvoir ajouter de
nouveaux objets à la base de données visuelle sans avoir à modifier notre code.

% réapprentissage
\paragraph{}
En discutant avec certains des nos encadrants de cours qui travaille sur la
thématique de l'apprentissage, nous avons pu apprendre que le principal effet
qui manquait à ceux que notre système produisait était l'effet de
réapprentissage qui veut que lorsque l'on a appris quelque chose, même si on
l'oublie car l'événement ne se reproduit plus, on pourra récupérer facilement
l'association qui a été perdue que si elle n'avait jamais été acquise. Pour
illustrer cet effet, on nous a cependant recommander d'utiliser deux modèles en
parallèle et un modèle final qui soit une combinaison des deux.
